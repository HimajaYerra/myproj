{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.models import Sequential\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('overnight_all_compressed.npz')\n",
    "eval_data = data['arr_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train data from zip file and merge all files in to single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.npyio.NpzFile object at 0x000002D43F1DBB70>\n",
      "(11044514, 4)\n",
      "(4769867, 4)\n",
      "(15814381, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in x:\\n    \\n    print(x[i].shape)\\n    temp=x[i]\\n    final_array = np.hstack()\\n    final_array = np.concatenate((final_array,temp),axis=0)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load('overnight_all_compressed_new.npz')\n",
    "print(x)\n",
    "x_t=[]\n",
    "final_array=np.array([])\n",
    "\n",
    "a = x['arr_0']\n",
    "b = x['arr_1']\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "\n",
    "final_array=np.vstack([a,b])\n",
    "\n",
    "print(final_array.shape)\n",
    "\n",
    "'''for i in x:\n",
    "    \n",
    "    print(x[i].shape)\n",
    "    temp=x[i]\n",
    "    final_array = np.hstack()\n",
    "    final_array = np.concatenate((final_array,temp),axis=0)'''\n",
    "    #np.concatenate(p,x[i])\n",
    "    #p =np.concatenate((p,temp),axis=0)\n",
    "    #np.append(x[i])\n",
    "    \n",
    "#data=['seconds','mic (trachea)','abdominal movement (stretch sensor)','thorax movement (stretch sensor)']\n",
    "#x = pd.DataFrame(x['arr_0'], columns = data)\n",
    "#x = pd.DataFrame(x['arr_1'], columns = data)\n",
    "\n",
    "#merged = list(itertools.chain.from_iterable(x_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into X & Y sets for the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = final_array[:,1]\n",
    "Y_train = final_array[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide X & Y sets for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time=eval_data[:100000,0]\n",
    "x_pred=eval_data[:100000,5].reshape(100000,1,1)\n",
    "y_true=eval_data[:100000,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 1 with 1D Convolution layers and final dense/FC layer with filter of size 1,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Conv_test():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(32,1,border_mode='same',activation='relu'))\n",
    "    model.add(Convolution1D(16,1,border_mode='same',activation='relu'))\n",
    "    model.add(Convolution1D(2,1,border_mode='same',activation='relu'))\n",
    "    model.add(Dense(1,activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 2 with 1D Convolution layers and final dense/FC layer with filter of size 3, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Conv1D_test():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64,3,padding='same',activation='relu'))\n",
    "    model.add(Conv1D(16,1,padding='same',activation='relu'))\n",
    "    model.add(Conv1D(2,1,padding='same',activation='relu'))\n",
    "    model.add(Dense(1,activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train data into train and validation using scikit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "    X_train,Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manually splitting train data  into train (70%) and validation (30% )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l= len(X_train)\n",
    "Xtrain = X_train[:round(l*0.7)]\n",
    "Xtest = X_train[round(l*0.7)+1:]\n",
    "ytrain = Y_train[:round(l*0.7)]\n",
    "ytest = Y_train[round(l*0.7)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10595635, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, 1, activation=\"relu\", padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\local\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(16, 1, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\local\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(2, 1, activation=\"relu\", padding=\"same\")`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10595635 samples, validate on 5218746 samples\n",
      "Epoch 1/100\n",
      " - 229s - loss: 0.3428 - acc: 0.0000e+00 - val_loss: 0.3422 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "\n",
    "Xtrain=(Xtrain).reshape(len(Xtrain),1,1)\n",
    "\n",
    "ytrain=ytrain.reshape(len(ytrain),1,1)\n",
    "\n",
    "Xtest=Xtest.reshape(len(Xtest),1,1)\n",
    "\n",
    "ytest=ytest.reshape(len(ytest),1,1)\n",
    "\n",
    "validationData=(Xtest,ytest)\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)'''\n",
    "print(Xtrain.shape)\n",
    "model=Conv_test()\n",
    "history = model.fit(Xtrain,ytrain,validation_data=validationData,epochs=100,batch_size=50,callbacks=[early_stopping],verbose=2)\n",
    "print(history.history.keys())\n",
    "#y_pred=model.predict(x_pred)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred=model.predict(x_pred)\n",
    "print(y_pred.shape)\n",
    "y_pred= y_pred.reshape(100000,1)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "#for ix,items in zip([1,2],x):\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.set_title('Time vs Patient Data')\n",
    "ax.set_xlabel('Time(in sec)')\n",
    "ax.set_ylabel('Patient Data')\n",
    "ax.plot(time,y_pred,label='abdominal movement (stretch sensor)_pred')\n",
    "ax.plot(time,y_true,label='abdominal movement (stretch sensor)_true')\n",
    "\n",
    "\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6af31080932a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtscv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtscv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_splits)\u001b[0m\n\u001b[0;32m    692\u001b[0m         super(TimeSeriesSplit, self).__init__(n_splits,\n\u001b[0;32m    693\u001b[0m                                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                                               random_state=None)\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\local\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    278\u001b[0m                 \u001b[1;34m\"k-fold cross-validation requires at least one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m                 \u001b[1;34m\" train/test split by setting n_splits=2 or more,\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \" got n_splits={0}.\".format(n_splits))\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=1)\n",
    "\n",
    "for train, test in tscv.split(final_array):\n",
    "    print(len(train))\n",
    "    print(len(test))\n",
    "    X_train=(final_array[train][:,1]).reshape(1,len(train),1)\n",
    "    #X_train=X_train.reshape(1,len(train),1)\n",
    "    Y_train=final_array[train][:,2].reshape(1,len(train),1)\n",
    "    #Y_train.reshape(1,len(train),1)\n",
    "    X_test=final_array[test][:,1].reshape(1,len(test),1)\n",
    "    #X_test.reshape(1,len(test),1)\n",
    "    Y_test=final_array[test][:,2].reshape(1,len(test),1)\n",
    "    #Y_test.reshape(1,len(test),1)\n",
    "    validationData=(X_test,Y_test)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_test.shape)\n",
    "    model=Conv_test()\n",
    "    history = model.fit(X_train,Y_train,epochs=50,batch_size=10, verbose=2)\n",
    "    print(history.history.keys())\n",
    "    #y_pred=model.predict(x_pred)\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    #plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#model.add(Embedding(max_features, 128))\n",
    "\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "\n",
    "              optimizer='adam',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "print('Train...')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "\n",
    "          batch_size=batch_size,\n",
    "\n",
    "          epochs=15,\n",
    "\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
